{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict audiobooks returning customers\n",
    "\n",
    "I am building a model that analysises data from customers of an audiobook app to classify them as returning or not, in a boolean way. By \"returning\" I mean returning customers, i.e. if they will go back to using the app to purchase more products.\n",
    "Methods used:\n",
    "- NN\n",
    "- SVM (as comparison)\n",
    "\n",
    "The data are imported from an attached csv file, and is tables as follows:\n",
    "\n",
    "| customer id | average minutes spent per book | total minutes spent on app  |average price of book   |total spent on app   |has left reviews?|review score|completion fraction| minutes listened |number of support requests|Last visited time minus purchase date| Target (dependent variable)|\n",
    "|---------------|-------|---|---|---|---|---|---|---|---|---|---|\n",
    "|   x   |   x    |  x | x  | x  |x|x|x|x|x|x|x|x|x|\n",
    "\n",
    "### Methodology - NN model\n",
    "\n",
    "- data loading from the npz files saved in the BC_preprocessing notebook;\n",
    "- model definition: NN with adjustable number of layers, nodes, etc;\n",
    "- definition of an alternative model with an early stopping mechanism, to avoid overfitting;\n",
    "- testing the model, and table with 2 examples of values obtained with slightly different models;\n",
    "\n",
    "\n",
    "This work is based on an exercise from https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a temporary variable npz, where we will store each of the three Audiobooks datasets\n",
    "npz = np.load('Audiobooks_data_train.npz')\n",
    "\n",
    "# we extract the inputs using the keyword under which we saved them\n",
    "# to ensure that they are all floats, let's also take care of that\n",
    "train_inputs = npz['inputs'].astype(np.float)\n",
    "# targets must be int because of sparse_categorical_crossentropy (we want to be able to smoothly one-hot encode them)\n",
    "train_targets = npz['targets'].astype(np.int)\n",
    "\n",
    "# we load the validation data in the temporary variable\n",
    "npz = np.load('Audiobooks_data_validation.npz')\n",
    "# we can load the inputs and the targets in the same line\n",
    "validation_inputs, validation_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n",
    "\n",
    "# we load the test data in the temporary variable\n",
    "npz = np.load('Audiobooks_data_test.npz')\n",
    "# we create 2 variables that will contain the test inputs and the test targets\n",
    "test_inputs, test_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "- we keep hl size as 50, we can change it afterwards;\n",
    "- we don't need the flatten method, as we already reprocessed the data\n",
    "- softmax is good for classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3579 samples, validate on 447 samples\n",
      "Epoch 1/100\n",
      "3579/3579 - 3s - loss: 0.5756 - accuracy: 0.7452 - val_loss: 0.4244 - val_accuracy: 0.8792\n",
      "Epoch 2/100\n",
      "3579/3579 - 0s - loss: 0.3863 - accuracy: 0.8664 - val_loss: 0.3174 - val_accuracy: 0.8881\n",
      "Epoch 3/100\n",
      "3579/3579 - 0s - loss: 0.3258 - accuracy: 0.8815 - val_loss: 0.2862 - val_accuracy: 0.8971\n",
      "Epoch 4/100\n",
      "3579/3579 - 0s - loss: 0.3013 - accuracy: 0.8885 - val_loss: 0.2693 - val_accuracy: 0.9038\n",
      "Epoch 5/100\n",
      "3579/3579 - 0s - loss: 0.2862 - accuracy: 0.8919 - val_loss: 0.2597 - val_accuracy: 0.9016\n",
      "Epoch 6/100\n",
      "3579/3579 - 0s - loss: 0.2774 - accuracy: 0.9003 - val_loss: 0.2522 - val_accuracy: 0.9016\n",
      "Epoch 7/100\n",
      "3579/3579 - 0s - loss: 0.2697 - accuracy: 0.9016 - val_loss: 0.2436 - val_accuracy: 0.9060\n",
      "Epoch 8/100\n",
      "3579/3579 - 0s - loss: 0.2640 - accuracy: 0.9033 - val_loss: 0.2419 - val_accuracy: 0.9105\n",
      "Epoch 9/100\n",
      "3579/3579 - 0s - loss: 0.2575 - accuracy: 0.9061 - val_loss: 0.2367 - val_accuracy: 0.9083\n",
      "Epoch 10/100\n",
      "3579/3579 - 0s - loss: 0.2534 - accuracy: 0.9047 - val_loss: 0.2345 - val_accuracy: 0.9150\n",
      "Epoch 11/100\n",
      "3579/3579 - 0s - loss: 0.2500 - accuracy: 0.9075 - val_loss: 0.2308 - val_accuracy: 0.9150\n",
      "Epoch 12/100\n",
      "3579/3579 - 0s - loss: 0.2481 - accuracy: 0.9081 - val_loss: 0.2343 - val_accuracy: 0.9128\n",
      "Epoch 13/100\n",
      "3579/3579 - 0s - loss: 0.2476 - accuracy: 0.9098 - val_loss: 0.2294 - val_accuracy: 0.9128\n",
      "Epoch 14/100\n",
      "3579/3579 - 0s - loss: 0.2422 - accuracy: 0.9078 - val_loss: 0.2312 - val_accuracy: 0.9172\n",
      "Epoch 15/100\n",
      "3579/3579 - 0s - loss: 0.2402 - accuracy: 0.9106 - val_loss: 0.2256 - val_accuracy: 0.9150\n",
      "Epoch 16/100\n",
      "3579/3579 - 0s - loss: 0.2377 - accuracy: 0.9125 - val_loss: 0.2288 - val_accuracy: 0.9128\n",
      "Epoch 17/100\n",
      "3579/3579 - 0s - loss: 0.2382 - accuracy: 0.9114 - val_loss: 0.2223 - val_accuracy: 0.9150\n",
      "Epoch 18/100\n",
      "3579/3579 - 0s - loss: 0.2352 - accuracy: 0.9125 - val_loss: 0.2219 - val_accuracy: 0.9172\n",
      "Epoch 19/100\n",
      "3579/3579 - 0s - loss: 0.2329 - accuracy: 0.9134 - val_loss: 0.2269 - val_accuracy: 0.9150\n",
      "Epoch 20/100\n",
      "3579/3579 - 0s - loss: 0.2353 - accuracy: 0.9120 - val_loss: 0.2248 - val_accuracy: 0.9150\n",
      "Epoch 21/100\n",
      "3579/3579 - 0s - loss: 0.2330 - accuracy: 0.9137 - val_loss: 0.2222 - val_accuracy: 0.9195\n",
      "Epoch 22/100\n",
      "3579/3579 - 0s - loss: 0.2322 - accuracy: 0.9137 - val_loss: 0.2195 - val_accuracy: 0.9172\n",
      "Epoch 23/100\n",
      "3579/3579 - 0s - loss: 0.2321 - accuracy: 0.9159 - val_loss: 0.2209 - val_accuracy: 0.9195\n",
      "Epoch 24/100\n",
      "3579/3579 - 0s - loss: 0.2287 - accuracy: 0.9156 - val_loss: 0.2221 - val_accuracy: 0.9172\n",
      "Epoch 25/100\n",
      "3579/3579 - 0s - loss: 0.2265 - accuracy: 0.9162 - val_loss: 0.2196 - val_accuracy: 0.9172\n",
      "Epoch 26/100\n",
      "3579/3579 - 0s - loss: 0.2271 - accuracy: 0.9167 - val_loss: 0.2190 - val_accuracy: 0.9195\n",
      "Epoch 27/100\n",
      "3579/3579 - 0s - loss: 0.2254 - accuracy: 0.9167 - val_loss: 0.2197 - val_accuracy: 0.9217\n",
      "Epoch 28/100\n",
      "3579/3579 - 0s - loss: 0.2239 - accuracy: 0.9167 - val_loss: 0.2226 - val_accuracy: 0.9172\n",
      "Epoch 29/100\n",
      "3579/3579 - 0s - loss: 0.2253 - accuracy: 0.9184 - val_loss: 0.2187 - val_accuracy: 0.9172\n",
      "Epoch 30/100\n",
      "3579/3579 - 0s - loss: 0.2229 - accuracy: 0.9187 - val_loss: 0.2209 - val_accuracy: 0.9195\n",
      "Epoch 31/100\n",
      "3579/3579 - 0s - loss: 0.2240 - accuracy: 0.9181 - val_loss: 0.2244 - val_accuracy: 0.9195\n",
      "Epoch 32/100\n",
      "3579/3579 - 0s - loss: 0.2234 - accuracy: 0.9162 - val_loss: 0.2197 - val_accuracy: 0.9195\n",
      "Epoch 33/100\n",
      "3579/3579 - 0s - loss: 0.2228 - accuracy: 0.9187 - val_loss: 0.2176 - val_accuracy: 0.9172\n",
      "Epoch 34/100\n",
      "3579/3579 - 0s - loss: 0.2223 - accuracy: 0.9187 - val_loss: 0.2184 - val_accuracy: 0.9172\n",
      "Epoch 35/100\n",
      "3579/3579 - 0s - loss: 0.2199 - accuracy: 0.9193 - val_loss: 0.2217 - val_accuracy: 0.9172\n",
      "Epoch 36/100\n",
      "3579/3579 - 0s - loss: 0.2236 - accuracy: 0.9167 - val_loss: 0.2290 - val_accuracy: 0.9150\n",
      "Epoch 37/100\n",
      "3579/3579 - 0s - loss: 0.2197 - accuracy: 0.9190 - val_loss: 0.2199 - val_accuracy: 0.9172\n",
      "Epoch 38/100\n",
      "3579/3579 - 0s - loss: 0.2193 - accuracy: 0.9190 - val_loss: 0.2213 - val_accuracy: 0.9150\n",
      "Epoch 39/100\n",
      "3579/3579 - 0s - loss: 0.2217 - accuracy: 0.9181 - val_loss: 0.2221 - val_accuracy: 0.9172\n",
      "Epoch 40/100\n",
      "3579/3579 - 0s - loss: 0.2182 - accuracy: 0.9193 - val_loss: 0.2255 - val_accuracy: 0.9128\n",
      "Epoch 41/100\n",
      "3579/3579 - 0s - loss: 0.2197 - accuracy: 0.9193 - val_loss: 0.2266 - val_accuracy: 0.9172\n",
      "Epoch 42/100\n",
      "3579/3579 - 0s - loss: 0.2185 - accuracy: 0.9209 - val_loss: 0.2203 - val_accuracy: 0.9150\n",
      "Epoch 43/100\n",
      "3579/3579 - 0s - loss: 0.2187 - accuracy: 0.9198 - val_loss: 0.2198 - val_accuracy: 0.9172\n",
      "Epoch 44/100\n",
      "3579/3579 - 0s - loss: 0.2215 - accuracy: 0.9190 - val_loss: 0.2303 - val_accuracy: 0.9172\n",
      "Epoch 45/100\n",
      "3579/3579 - 0s - loss: 0.2193 - accuracy: 0.9190 - val_loss: 0.2229 - val_accuracy: 0.9172\n",
      "Epoch 46/100\n",
      "3579/3579 - 0s - loss: 0.2162 - accuracy: 0.9204 - val_loss: 0.2187 - val_accuracy: 0.9150\n",
      "Epoch 47/100\n",
      "3579/3579 - 0s - loss: 0.2163 - accuracy: 0.9187 - val_loss: 0.2214 - val_accuracy: 0.9172\n",
      "Epoch 48/100\n",
      "3579/3579 - 0s - loss: 0.2155 - accuracy: 0.9201 - val_loss: 0.2238 - val_accuracy: 0.9195\n",
      "Epoch 49/100\n",
      "3579/3579 - 0s - loss: 0.2177 - accuracy: 0.9201 - val_loss: 0.2222 - val_accuracy: 0.9172\n",
      "Epoch 50/100\n",
      "3579/3579 - 0s - loss: 0.2155 - accuracy: 0.9204 - val_loss: 0.2224 - val_accuracy: 0.9150\n",
      "Epoch 51/100\n",
      "3579/3579 - 0s - loss: 0.2164 - accuracy: 0.9209 - val_loss: 0.2224 - val_accuracy: 0.9172\n",
      "Epoch 52/100\n",
      "3579/3579 - 0s - loss: 0.2164 - accuracy: 0.9198 - val_loss: 0.2226 - val_accuracy: 0.9150\n",
      "Epoch 53/100\n",
      "3579/3579 - 0s - loss: 0.2130 - accuracy: 0.9215 - val_loss: 0.2200 - val_accuracy: 0.9195\n",
      "Epoch 54/100\n",
      "3579/3579 - 0s - loss: 0.2166 - accuracy: 0.9204 - val_loss: 0.2239 - val_accuracy: 0.9150\n",
      "Epoch 55/100\n",
      "3579/3579 - 0s - loss: 0.2130 - accuracy: 0.9206 - val_loss: 0.2218 - val_accuracy: 0.9150\n",
      "Epoch 56/100\n",
      "3579/3579 - 0s - loss: 0.2171 - accuracy: 0.9206 - val_loss: 0.2244 - val_accuracy: 0.9172\n",
      "Epoch 57/100\n",
      "3579/3579 - 0s - loss: 0.2155 - accuracy: 0.9195 - val_loss: 0.2252 - val_accuracy: 0.9150\n",
      "Epoch 58/100\n",
      "3579/3579 - 0s - loss: 0.2175 - accuracy: 0.9195 - val_loss: 0.2240 - val_accuracy: 0.9195\n",
      "Epoch 59/100\n",
      "3579/3579 - 0s - loss: 0.2125 - accuracy: 0.9215 - val_loss: 0.2240 - val_accuracy: 0.9172\n",
      "Epoch 60/100\n",
      "3579/3579 - 0s - loss: 0.2131 - accuracy: 0.9218 - val_loss: 0.2209 - val_accuracy: 0.9150\n",
      "Epoch 61/100\n",
      "3579/3579 - 0s - loss: 0.2113 - accuracy: 0.9212 - val_loss: 0.2220 - val_accuracy: 0.9150\n",
      "Epoch 62/100\n",
      "3579/3579 - 0s - loss: 0.2107 - accuracy: 0.9234 - val_loss: 0.2305 - val_accuracy: 0.9172\n",
      "Epoch 63/100\n",
      "3579/3579 - 0s - loss: 0.2155 - accuracy: 0.9201 - val_loss: 0.2209 - val_accuracy: 0.9150\n",
      "Epoch 64/100\n",
      "3579/3579 - 0s - loss: 0.2144 - accuracy: 0.9215 - val_loss: 0.2271 - val_accuracy: 0.9128\n",
      "Epoch 65/100\n",
      "3579/3579 - 0s - loss: 0.2119 - accuracy: 0.9215 - val_loss: 0.2283 - val_accuracy: 0.9217\n",
      "Epoch 66/100\n",
      "3579/3579 - 0s - loss: 0.2123 - accuracy: 0.9229 - val_loss: 0.2210 - val_accuracy: 0.9172\n",
      "Epoch 67/100\n",
      "3579/3579 - 0s - loss: 0.2114 - accuracy: 0.9223 - val_loss: 0.2203 - val_accuracy: 0.9150\n",
      "Epoch 68/100\n",
      "3579/3579 - 0s - loss: 0.2110 - accuracy: 0.9215 - val_loss: 0.2214 - val_accuracy: 0.9172\n",
      "Epoch 69/100\n",
      "3579/3579 - 0s - loss: 0.2098 - accuracy: 0.9223 - val_loss: 0.2214 - val_accuracy: 0.9172\n",
      "Epoch 70/100\n",
      "3579/3579 - 0s - loss: 0.2114 - accuracy: 0.9209 - val_loss: 0.2280 - val_accuracy: 0.9172\n",
      "Epoch 71/100\n",
      "3579/3579 - 0s - loss: 0.2100 - accuracy: 0.9215 - val_loss: 0.2247 - val_accuracy: 0.9083\n",
      "Epoch 72/100\n",
      "3579/3579 - 0s - loss: 0.2103 - accuracy: 0.9226 - val_loss: 0.2221 - val_accuracy: 0.9150\n",
      "Epoch 73/100\n",
      "3579/3579 - 0s - loss: 0.2123 - accuracy: 0.9226 - val_loss: 0.2255 - val_accuracy: 0.9172\n",
      "Epoch 74/100\n",
      "3579/3579 - 0s - loss: 0.2112 - accuracy: 0.9232 - val_loss: 0.2280 - val_accuracy: 0.9195\n",
      "Epoch 75/100\n",
      "3579/3579 - 0s - loss: 0.2133 - accuracy: 0.9223 - val_loss: 0.2281 - val_accuracy: 0.9016\n",
      "Epoch 76/100\n",
      "3579/3579 - 0s - loss: 0.2082 - accuracy: 0.9223 - val_loss: 0.2280 - val_accuracy: 0.9172\n",
      "Epoch 77/100\n",
      "3579/3579 - 0s - loss: 0.2073 - accuracy: 0.9229 - val_loss: 0.2196 - val_accuracy: 0.9172\n",
      "Epoch 78/100\n",
      "3579/3579 - 0s - loss: 0.2074 - accuracy: 0.9229 - val_loss: 0.2241 - val_accuracy: 0.9060\n",
      "Epoch 79/100\n",
      "3579/3579 - 0s - loss: 0.2093 - accuracy: 0.9220 - val_loss: 0.2257 - val_accuracy: 0.9172\n",
      "Epoch 80/100\n",
      "3579/3579 - 0s - loss: 0.2076 - accuracy: 0.9226 - val_loss: 0.2339 - val_accuracy: 0.8949\n",
      "Epoch 81/100\n",
      "3579/3579 - 0s - loss: 0.2077 - accuracy: 0.9232 - val_loss: 0.2223 - val_accuracy: 0.9172\n",
      "Epoch 82/100\n",
      "3579/3579 - 0s - loss: 0.2077 - accuracy: 0.9218 - val_loss: 0.2223 - val_accuracy: 0.9172\n",
      "Epoch 83/100\n",
      "3579/3579 - 0s - loss: 0.2093 - accuracy: 0.9218 - val_loss: 0.2298 - val_accuracy: 0.9016\n",
      "Epoch 84/100\n",
      "3579/3579 - 0s - loss: 0.2078 - accuracy: 0.9229 - val_loss: 0.2223 - val_accuracy: 0.9172\n",
      "Epoch 85/100\n",
      "3579/3579 - 0s - loss: 0.2116 - accuracy: 0.9226 - val_loss: 0.2239 - val_accuracy: 0.9150\n",
      "Epoch 86/100\n",
      "3579/3579 - 0s - loss: 0.2107 - accuracy: 0.9240 - val_loss: 0.2241 - val_accuracy: 0.9172\n",
      "Epoch 87/100\n",
      "3579/3579 - 0s - loss: 0.2088 - accuracy: 0.9232 - val_loss: 0.2226 - val_accuracy: 0.9195\n",
      "Epoch 88/100\n",
      "3579/3579 - 0s - loss: 0.2069 - accuracy: 0.9223 - val_loss: 0.2242 - val_accuracy: 0.9150\n",
      "Epoch 89/100\n",
      "3579/3579 - 0s - loss: 0.2056 - accuracy: 0.9226 - val_loss: 0.2203 - val_accuracy: 0.9172\n",
      "Epoch 90/100\n",
      "3579/3579 - 0s - loss: 0.2043 - accuracy: 0.9237 - val_loss: 0.2300 - val_accuracy: 0.8993\n",
      "Epoch 91/100\n",
      "3579/3579 - 0s - loss: 0.2076 - accuracy: 0.9215 - val_loss: 0.2251 - val_accuracy: 0.9172\n",
      "Epoch 92/100\n",
      "3579/3579 - 0s - loss: 0.2095 - accuracy: 0.9215 - val_loss: 0.2408 - val_accuracy: 0.8971\n",
      "Epoch 93/100\n",
      "3579/3579 - 0s - loss: 0.2094 - accuracy: 0.9243 - val_loss: 0.2240 - val_accuracy: 0.9195\n",
      "Epoch 94/100\n",
      "3579/3579 - 0s - loss: 0.2058 - accuracy: 0.9251 - val_loss: 0.2229 - val_accuracy: 0.9195\n",
      "Epoch 95/100\n",
      "3579/3579 - 0s - loss: 0.2097 - accuracy: 0.9209 - val_loss: 0.2435 - val_accuracy: 0.8971\n",
      "Epoch 96/100\n",
      "3579/3579 - 0s - loss: 0.2092 - accuracy: 0.9229 - val_loss: 0.2284 - val_accuracy: 0.9038\n",
      "Epoch 97/100\n",
      "3579/3579 - 0s - loss: 0.2061 - accuracy: 0.9223 - val_loss: 0.2254 - val_accuracy: 0.9172\n",
      "Epoch 98/100\n",
      "3579/3579 - 0s - loss: 0.2070 - accuracy: 0.9226 - val_loss: 0.2258 - val_accuracy: 0.9172\n",
      "Epoch 99/100\n",
      "3579/3579 - 0s - loss: 0.2088 - accuracy: 0.9234 - val_loss: 0.2344 - val_accuracy: 0.8904\n",
      "Epoch 100/100\n",
      "3579/3579 - 0s - loss: 0.2064 - accuracy: 0.9243 - val_loss: 0.2237 - val_accuracy: 0.9172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x183e46ffdc8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 10\n",
    "output_size = 2\n",
    "# Use same hidden layer size for both hidden layers. Not a necessity.\n",
    "hidden_layer_size = 50\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    \n",
    "    # the first layer (the input layer)\n",
    "    # each observation is 28x28x1 pixels, therefore it is a tensor of rank 3\n",
    "    # since we don't know CNNs yet, we don't know how to feed such input into our net, so we must flatten the images\n",
    "    # there is a convenient method 'Flatten' that simply takes our 28x28x1 tensor and orders it into a (None,) \n",
    "    # or (28x28x1,) = (784,) vector\n",
    "    # this allows us to actually create a feed forward neural network\n",
    "    #tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # input layer\n",
    "    \n",
    "    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n",
    "    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
    "    #tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 3rd hidden layer\n",
    "\n",
    "    # will have to explore combinations of activation functions\n",
    "    \n",
    "    # the final layer is no different, we just make sure to activate it with softmax\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "batch_size=100\n",
    "max_epochs=100\n",
    "\n",
    "model.fit(train_inputs, \n",
    "          train_targets, \n",
    "          batch_size=batch_size, \n",
    "          epochs =max_epochs, \n",
    "          validation_data=(validation_inputs, validation_targets),\n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the validation loss oscillates, which means that we have some overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting an early stopping mechanism\n",
    "opeating on the fit method input _callback_:\n",
    "\n",
    "functions called during the model training, many available, we are using EarlyStopping.\n",
    "\n",
    "tf.keras.callbacks.EarlyStopping(patience): patience by default is set to 0, but we can set how many consecutive val_loss increases we can tolerate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3579 samples, validate on 447 samples\n",
      "Epoch 1/100\n",
      "3579/3579 - 1s - loss: 0.5666 - accuracy: 0.7368 - val_loss: 0.3859 - val_accuracy: 0.8926\n",
      "Epoch 2/100\n",
      "3579/3579 - 0s - loss: 0.3611 - accuracy: 0.8751 - val_loss: 0.2846 - val_accuracy: 0.8993\n",
      "Epoch 3/100\n",
      "3579/3579 - 0s - loss: 0.3054 - accuracy: 0.8894 - val_loss: 0.2646 - val_accuracy: 0.9016\n",
      "Epoch 4/100\n",
      "3579/3579 - 0s - loss: 0.2836 - accuracy: 0.8991 - val_loss: 0.2533 - val_accuracy: 0.9038\n",
      "Epoch 5/100\n",
      "3579/3579 - 0s - loss: 0.2692 - accuracy: 0.9005 - val_loss: 0.2457 - val_accuracy: 0.9038\n",
      "Epoch 6/100\n",
      "3579/3579 - 0s - loss: 0.2599 - accuracy: 0.9030 - val_loss: 0.2446 - val_accuracy: 0.9083\n",
      "Epoch 7/100\n",
      "3579/3579 - 0s - loss: 0.2566 - accuracy: 0.9058 - val_loss: 0.2351 - val_accuracy: 0.9060\n",
      "Epoch 8/100\n",
      "3579/3579 - 0s - loss: 0.2509 - accuracy: 0.9044 - val_loss: 0.2332 - val_accuracy: 0.9128\n",
      "Epoch 9/100\n",
      "3579/3579 - 0s - loss: 0.2428 - accuracy: 0.9098 - val_loss: 0.2341 - val_accuracy: 0.9172\n",
      "Epoch 10/100\n",
      "3579/3579 - 0s - loss: 0.2438 - accuracy: 0.9145 - val_loss: 0.2292 - val_accuracy: 0.9172\n",
      "Epoch 11/100\n",
      "3579/3579 - 0s - loss: 0.2385 - accuracy: 0.9142 - val_loss: 0.2268 - val_accuracy: 0.9150\n",
      "Epoch 12/100\n",
      "3579/3579 - 0s - loss: 0.2364 - accuracy: 0.9137 - val_loss: 0.2409 - val_accuracy: 0.9150\n",
      "Epoch 13/100\n",
      "3579/3579 - 0s - loss: 0.2335 - accuracy: 0.9139 - val_loss: 0.2226 - val_accuracy: 0.9150\n",
      "Epoch 14/100\n",
      "3579/3579 - 0s - loss: 0.2316 - accuracy: 0.9159 - val_loss: 0.2245 - val_accuracy: 0.9172\n",
      "Epoch 15/100\n",
      "3579/3579 - 0s - loss: 0.2277 - accuracy: 0.9167 - val_loss: 0.2295 - val_accuracy: 0.9172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x183ee2277c8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 10\n",
    "output_size = 2\n",
    "# Use same hidden layer size for both hidden layers. Not a necessity.\n",
    "hidden_layer_size = 50\n",
    "\n",
    "model_es = tf.keras.Sequential([\n",
    "    \n",
    "    \n",
    "    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n",
    "    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 3rd hidden layer\n",
    "\n",
    "    # will have to explore combinations of activation functions\n",
    "    \n",
    "    # the final layer is no different, we just make sure to activate it with softmax\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "])\n",
    "\n",
    "model_es.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "batch_size=100\n",
    "max_epochs=100\n",
    "\n",
    "early_stopping=tf.keras.callbacks.EarlyStopping(patience=2) #stops as val acc decreases, the first time\n",
    "\n",
    "model_es.fit(train_inputs, \n",
    "          train_targets, \n",
    "          batch_size=batch_size, \n",
    "          epochs =max_epochs, \n",
    "          callbacks = [early_stopping],\n",
    "          validation_data=(validation_inputs, validation_targets),\n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val_accuracy: 0.9150; Awesome!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 320us/sample - loss: 0.2965 - accuracy: 0.8862\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model_es.evaluate(test_inputs, test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0s 476us/sample - loss: 0.2887 - accuracy: 0.8906"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: trying the get the accuracy better\n",
    "\n",
    "| hidden layers | nodes | activation  |batch size   |optimizer   |loss|accuracy|\n",
    "|---------------|-------|---|---|---|---|---|\n",
    "|        2       |   50    |  relu | 100  | adam  |0.2887|0.8906|\n",
    "|               |    100   |   |   |   |0.3059|0.8862|\n",
    "|         3      |   50    |   |   |   |0.2965|0.8862|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
